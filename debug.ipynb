{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example to use safetensor with stable diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/disk6/chrenx/conda-env/envs/humanai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from diffusers.utils import logging\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "import torch\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "prompt = \"Sherlock Holmes\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "safetensor_path = 'models/colorwater-v4.safetensors'\n",
    "\n",
    "before = torch.cuda.get_device_properties(7).total_memory - torch.cuda.memory_allocated(7)\n",
    "before /= (1024 ** 2)\n",
    "#*********************\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_single_file(safetensor_path,\n",
    "                                                    extract_ema=True,\n",
    "                                                    safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "                                                    feature_extractor=CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
    "                                                    image_size=512)\n",
    "# device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "# pipeline.to(device)\n",
    "\n",
    "# print(pipeline.device)\n",
    "\n",
    "# image = pipeline(\n",
    "#     prompt=prompt,\n",
    "#     negative_prompt=negative_prompt, \n",
    "#     num_inference_steps=150, # sampling steps\n",
    "#     guidance_scale= 7, # 7, # cfg\n",
    "#     generator=torch.manual_seed(246372),\n",
    "# )\n",
    "\n",
    "# after = torch.cuda.get_device_properties(7).total_memory - torch.cuda.memory_allocated(7)\n",
    "# after /= (1024 ** 2)\n",
    "\n",
    "# print(f\"usage: {(before-after):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.27.2\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"DDIMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "# pipeline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1024, 1024), PIL.Image.Image)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = image.images[0]\n",
    "img.save(\"output.png\")\n",
    "if image['nsfw_content_detected'][0]:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "img.size, type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500000000.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 7.5e+9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://cf03891c9a0af8d165.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cf03891c9a0af8d165.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def show_text(model_name):\n",
    "    return gr.update()\n",
    "\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # Show text!\n",
    "    Start typing below to see the output.\n",
    "    \"\"\"\n",
    "    )\n",
    "    model_name = gr.Dropdown(label=\"Select Your Imaging Style\", \n",
    "                                         choices=[2, 4, 5, 3], value=None)\n",
    "\n",
    "    model_name.change(fn=show_text, inputs=model_name, outputs=model_name, show_progress = True)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
