{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(0, 9, size=(40000, 35000))\n",
    "a = a.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 25388515328)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(0), torch.cuda.get_device_properties(0).total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 25769803776\n",
      "free     : 8190820352\n",
      "used     : 17578983424\n",
      "0.3178456624348958\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')\n",
    "print(info.free / info.total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/disk5/chrenx/conda-env/envs/humanai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from diffusers.utils import logging\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "import torch\n",
    "import safetensors\n",
    "\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Sherlock Holmes\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "safetensor_path = 'models/pencil_sketch.safetensors'\n",
    "\n",
    "device_id = 2\n",
    "\n",
    "before = torch.cuda.get_device_properties(device_id).total_memory - torch.cuda.memory_allocated(7)\n",
    "before /= (1024 ** 2)\n",
    "#*********************\n",
    "\n",
    "# pipeline = DiffusionPipeline.from_pretrained(safetensor_path, torch_dtype=torch.float16, use_safetensors=True)\n",
    "\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_single_file(safetensor_path,\n",
    "                                                    extract_ema=True,\n",
    "                                                    safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "                                                    feature_extractor=CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
    "                                                    image_size=512)\n",
    "\n",
    "pipeline.to(f\"cuda:{device_id}\")\n",
    "\n",
    "after = torch.cuda.get_device_properties(device_id).total_memory - torch.cuda.memory_allocated(7)\n",
    "after /= (1024 ** 2)\n",
    "\n",
    "print(f\"usage: {(before-after):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example to use safetensor with stable diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: 0.25 MB\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from diffusers.utils import logging\n",
    "from transformers import CLIPImageProcessor\n",
    "from PIL import Image \n",
    "\n",
    "import torch\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "prompt = \"asdqwe\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "safetensor_path = \"models/background_beautiful_outdoor.safetensors\"\n",
    "\n",
    "device_id = 1\n",
    "\n",
    "before = torch.cuda.get_device_properties(device_id).total_memory - torch.cuda.memory_allocated(device_id)\n",
    "before /= (1024 ** 2)\n",
    "#*********************\n",
    "\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_single_file(safetensor_path,\n",
    "                                                    extract_ema=True,\n",
    "                                                    safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "                                                    feature_extractor=CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
    "                                                    image_size=512)\n",
    "device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline.to(device)\n",
    "\n",
    "print(pipeline.device)\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt, \n",
    "    num_inference_steps=25, # sampling steps\n",
    "    guidance_scale=7, # 7, # cfg\n",
    "    generator=torch.manual_seed(246372),\n",
    "    num_images_per_prompt=2,\n",
    "    width= 384, # 360,\n",
    "    height= 512, # 480\n",
    ")\n",
    "\n",
    "img0 = image.images[0].copy()\n",
    "img1 = image.images[1].copy()\n",
    "\n",
    "after = torch.cuda.get_device_properties(device_id).total_memory - torch.cuda.memory_allocated(device_id)\n",
    "after /= (1024 ** 2)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"usage: {(before-after):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image['nsfw_content_detected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(image.images)):\n",
    "    img = image.images[i]\n",
    "    img.save(f\"output{i}.png\")\n",
    "    if image['nsfw_content_detected'][0]:\n",
    "        print('yes')\n",
    "    else:\n",
    "        print('no')\n",
    "# img = image.images[0]\n",
    "# img.save(\"output.png\")\n",
    "# if image['nsfw_content_detected'][0]:\n",
    "#     print('yes')\n",
    "# else:\n",
    "#     print('no')\n",
    "# img.size, type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500000000.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 4, 5]\n",
    "b = 3\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://cf03891c9a0af8d165.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cf03891c9a0af8d165.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def show_text(model_name):\n",
    "    return gr.update()\n",
    "\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # Show text!\n",
    "    Start typing below to see the output.\n",
    "    \"\"\"\n",
    "    )\n",
    "    model_name = gr.Dropdown(label=\"Select Your Imaging Style\", \n",
    "                                         choices=[2, 4, 5, 3], value=None)\n",
    "\n",
    "    model_name.change(fn=show_text, inputs=model_name, outputs=model_name, show_progress = True)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humanai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
